<!DOCTYPE html>
<!-- saved from url=(0062)https://witestlab.poly.edu/blog/tcp-congestion-control-basics/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>TCP congestion control</title>
    <meta name="description" content="">

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="shortcut icon" href="https://witestlab.poly.edu/blog/favicon.ico">

    <link rel="stylesheet" type="text/css" href="./TCP congestion control_files/screen.css">
    <link rel="stylesheet" type="text/css" href="./TCP congestion control_files/css">

    <link rel="canonical" href="http://witestlab.poly.edu/blog/tcp-congestion-control-basics/">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="http://witestlab.poly.edu/blog/tcp-congestion-control-basics/amp/">
    
    <meta property="og:site_name" content="Run my testbed experiment">
    <meta property="og:type" content="article">
    <meta property="og:title" content="TCP congestion control">
    <meta property="og:description" content="This experiment shows the basic behavior of TCP congestion control. You&#39;ll see the classic &quot;sawtooth&quot; pattern in a TCP flow&#39;s congestion window, and you&#39;ll see how a TCP flow responds to congestion indicators. It should take about 1 hour to run this experiment. You can run this experiment on GENI">
    <meta property="og:url" content="http://witestlab.poly.edu/blog/tcp-congestion-control-basics/">
    <meta property="og:image" content="http://witestlab.poly.edu/blog/content/images/2017/04/tcp-one-1.svg">
    <meta property="article:published_time" content="2017-04-10T21:53:27.000Z">
    <meta property="article:modified_time" content="2022-10-27T13:41:56.000Z">
    <meta property="article:tag" content="tcp">
    <meta property="article:tag" content="transport layer">
    <meta property="article:tag" content="education">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="TCP congestion control">
    <meta name="twitter:description" content="This experiment shows the basic behavior of TCP congestion control. You&#39;ll see the classic &quot;sawtooth&quot; pattern in a TCP flow&#39;s congestion window, and you&#39;ll see how a TCP flow responds to congestion indicators. It should take about 1 hour to run this experiment. You can run this experiment on GENI">
    <meta name="twitter:url" content="http://witestlab.poly.edu/blog/tcp-congestion-control-basics/">
    <meta name="twitter:image" content="http://witestlab.poly.edu/blog/content/images/2017/04/tcp-one-1.svg">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Fraida Fund">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="tcp, transport layer, education">
    
    <script async="" src="./TCP congestion control_files/analytics.js.download"></script><script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Run my testbed experiment",
        "logo": "http://witestlab.poly.edu/blog/ghost/img/ghosticon.jpg"
    },
    "author": {
        "@type": "Person",
        "name": "Fraida Fund",
        "image": "http://witestlab.poly.edu/blog/content/images/2016/01/head.jpg",
        "url": "http://witestlab.poly.edu/blog/author/ffund/",
        "sameAs": [
            "http://witestlab.poly.edu/~ffund/"
        ]
    },
    "headline": "TCP congestion control",
    "url": "https://witestlab.poly.edu/blog/tcp-congestion-control-basics/",
    "datePublished": "2017-04-10T21:53:27.000Z",
    "dateModified": "2022-10-27T13:41:56.000Z",
    "image": "http://witestlab.poly.edu/blog/content/images/2017/04/tcp-one-1.svg",
    "keywords": "tcp, transport layer, education",
    "description": "This experiment shows the basic behavior of TCP congestion control. You&#x27;ll see the classic &quot;sawtooth&quot; pattern in a TCP flow&#x27;s congestion window, and you&#x27;ll see how a TCP flow responds to congestion indicators. It should take about 1 hour to run this experiment. You can run this experiment on GENI",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://witestlab.poly.edu/blog"
    }
}
    </script>

    <meta name="generator" content="Ghost 0.10">
    <link rel="alternate" type="application/rss+xml" title="Run my testbed experiment" href="https://witestlab.poly.edu/blog/rss/">
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-25309460-4', 'auto');
  ga('send', 'pageview');

</script>

<link href="./TCP congestion control_files/css(1)" rel="stylesheet" type="text/css">
<link href="./TCP congestion control_files/css(2)" rel="stylesheet" type="text/css">
<style>
body,
h1, h2, h3, h4, h5, h6,
.main-nav a,
.subscribe-button,
.page-title,
.post-meta,
.read-next-story .post:before,
.pagination,
.site-footer {
    font-family:"Fira Sans", sans-serif; /* Replace with your own font */
}
    
.site-footer{
    font-size: 1.2rem;
}
.main-header {
   background-color:#0e7d8e!important; /*Replace with your own 6-digit hex color */
}
    
 .nav {
   background-color:#816c5d!important; /*Replace with your own 6-digit hex color */
}
    
 .nav-opened .menu-button {
	opacity: 0;
   }
h1, h2, h3{
	color:#818790;
}
    
 .post-title{
     color:#4A4A4A;
     margin-top: 20px;
 }
  .st-alert code{border:none;background:rgba(0,0,0,0.2);}.st-alert a{color:#2f8cde;}.st-alert-info a,.st-alert-info a:visited,.st-alert-info a:hover{color:#fff}
.st-alert{padding:20px 30px;margin-bottom:20px;ttext-shadow:0 1px 0 rgba(255,255,255,0.3);background-color:#F4CCCC;position:relative;color:#484019;-webkit-border-radius:1px;-moz-border-radius:1px;border-radius:1px;}.st-alert-success{background-color:#0e7d8e;color:#445028;}.st-alert-danger,.st-alert-error{background-color:#0e7d8e;color:#fff;}.st-alert-info{background-color:#0e7d8e;color:#fff;}.st-alert span{position:absolute;left:0;top:0;padding:2px 8px;color:#fff;font-size:13px;background:rgba(0,0,0,0.2);-webkit-border-radius:1px 0 1px 0;-moz-border-radius:1px 0 1px 0;border-radius:1px 0 1px 0;}.st-alert.with_title{padding:40px 25px 30px 60px;}

 .geni-specific {
  border-color:#FB8C00; 
  border-style:solid; 
  padding: 15px;
}
h4.geni-specific {
  color:#FB8C00; 
  border-style: none; 
  padding: 0px;

}
.cloudlab-specific {
  border-color:#5e8a90; 
  border-style:solid; 
  padding: 15px;
}
h4.cloudlab-specific {
  color:#5e8a90; 
  border-style: none; 
  padding: 0px;

}
.fabric-specific {
  border-color:#47aae1; 
  border-style:solid; 
  padding: 15px;
}
h4.fabric-specific {
  color:#47aae1; 
  border-style: none; 
  padding: 0px;

}


</style>

<link href="./TCP congestion control_files/prism.css" rel="stylesheet">
<script src="./TCP congestion control_files/prism.js.download"></script>


<script type="text/javascript" async="" src="./TCP congestion control_files/MathJax.js.download">
</script>
<script src="./TCP congestion control_files/embed.js.download" data-timestamp="1668552165574"></script><style id="fit-vids-style">.fluid-width-video-wrapper{width:100%;position:relative;padding:0;}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper object,.fluid-width-video-wrapper embed {position:absolute;top:0;left:0;width:100%;height:100%;}</style><link rel="prefetch" as="style" href="https://c.disquscdn.com/next/embed/styles/lounge.a6de6ea97286e24f0dd4753382b1e015.css"><link rel="prefetch" as="script" href="https://c.disquscdn.com/next/embed/common.bundle.a0092a9b6d9c06bf965e6c41a81f2c09.js"><link rel="prefetch" as="script" href="https://c.disquscdn.com/next/embed/lounge.bundle.5a2e28d99014bd8296f13769d50defae.js"><link rel="prefetch" as="script" href="https://disqus.com/next/config.js"><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><script src="./TCP congestion control_files/alfie_v4.63f1ab6d6b9d5807dc0c94ef3fe0b851.js.download" async="" charset="UTF-8"></script></head>
<body class="post-template tag-tcp tag-transport-layer tag-education nav-closed"><div id="MathJax_Message" style="display: none;"></div>

    <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="https://witestlab.poly.edu/blog/tcp-congestion-control-basics/#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
            <li class="nav-home" role="presentation"><a href="https://witestlab.poly.edu/blog/">Home</a></li>
            <li class="nav-about" role="presentation"><a href="https://witestlab.poly.edu/blog/about">About</a></li>
            <li class="nav-topics" role="presentation"><a href="https://witestlab.poly.edu/blog/topics">Topics</a></li>
    </ul>
        <a class="subscribe-button icon-feed" href="https://witestlab.poly.edu/blog/rss/">Subscribe</a>
</div>
<span class="nav-cover"></span>


    <div class="site-wrapper">

        


<header class="main-header post-head " style="background-image: url(/blog/content/images/2017/04/tcp-one-1.svg)">
    <nav class="main-nav overlay clearfix">
        
            <a class="menu-button icon-menu" href="https://witestlab.poly.edu/blog/tcp-congestion-control-basics/#"><span class="word">Menu</span></a>
    </nav>
</header>

<main class="content" role="main">
    <article class="post tag-tcp tag-transport-layer tag-education">

        <header class="post-header">
            <h1 class="post-title">TCP congestion control</h1>
            <section class="post-meta">
                    <a href="https://witestlab.poly.edu/blog/author/ffund/">Fraida Fund</a>
                <br>
                <time class="post-date" datetime="2017-04-10">10 April 2017</time>  on <a href="https://witestlab.poly.edu/blog/tag/tcp/">tcp</a>, <a href="https://witestlab.poly.edu/blog/tag/transport-layer/">transport layer</a>, <a href="https://witestlab.poly.edu/blog/tag/education/">education</a>
            </section>
        </header>

        <section class="post-content">
            <p>This experiment shows the basic behavior of TCP congestion control. You'll see the classic "sawtooth" pattern in a TCP flow's congestion window, and you'll see how a TCP flow responds to congestion indicators.</p>

<p>It should take about 1 hour to run this experiment.</p>

<p>You can run this experiment on GENI or on CloudLab. Refer to the testbed-specific prerequisites listed below.</p>

<div style="border-color:#FB8C00; border-style:solid; padding: 15px;">  
<h4 style="color:#FB8C00;"> GENI-specific instructions: Prerequisites</h4>

To reproduce this experiment on GENI, you will need an account on the <a href="http://groups.geni.net/geni/wiki/SignMeUp">GENI Portal</a>, and you will need to have <a href="http://groups.geni.net/geni/wiki/JoinAProject">joined a project</a>. You should have already <a href="http://groups.geni.net/geni/wiki/HowTo/LoginToNodes">uploaded your SSH keys to the portal and know how to log in to a node with those keys</a>.  
</div>  

<p><br></p>

<div style="border-color:#5e8a90; border-style:solid; padding: 15px;">  
<h4 style="color:#5e8a90;"> Cloudlab-specific instructions: Prerequisites</h4>

To reproduce this experiment on Cloudlab, you will need an account on <a href="https://cloudlab.us/">Cloudlab</a>, you will need to have <a href="https://docs.cloudlab.us/users.html#%28part._join-project%29">joined a project</a>, and you will need to have <a href="https://docs.cloudlab.us/users.html#%28part._ssh-access%29">set up SSH access</a>.

</div>  

<p><br></p>

<ul>
<li>Skip to <a href="https://witestlab.poly.edu/blog/tcp-congestion-control-basics/#results">Results</a></li>
<li>Skip to <a href="https://witestlab.poly.edu/blog/tcp-congestion-control-basics/#runmyexperiment">Run my experiment</a></li>
</ul>

<h2 id="background">Background</h2>

<p>TCP is arguably one of the most important Internet protocols, as it carries a much higher volume of traffic on the Internet than any other transport-layer protocol.</p>

<p>Because TCP carries so much traffic, its congestion control algorithm is the main technique which prevents the Internet from slowing to a crawl due to over-utilization. This is a state known as <em>congestion collapse</em>, and occurs when the link is "busy" but not getting useful work done - for example, if the network is clogged with unnecessary retransmissions of lost packets.</p>

<p>In fact, an Internet collapse <em>has</em> occurred in the past, due to insufficient congestion control. A <a href="https://doi.org/10.1145/52325.52356">1988 paper by Van Jacobson explains</a>:</p>

<blockquote>
  <p>In October of '86, the Internet had the first of what became a series of 'congestion collapses.' During this period, the data throughput from LBL to UC Berkeley (sites separated by 400 yards and three IMP hops) dropped from 32 Kbps to 40 bps.</p>
</blockquote>

<p>The effective throughput (the volume of useful data transferred over the link) dropped by a factor of ~1000!</p>

<p>Because TCP congestion control is so essential, it is also continuously undergoing improvement and refinement. As the Internet (and the characteristics of Internet traffic) changes, TCP congestion control must evolve along with it. However, in this experiment, we'll examine an early variant of TCP congestion control that uses a basic <em>additive increase, multiplicative decrease</em> rule.</p>

<h3 id="aimdadditiveincreasemultiplicativedecrease">AIMD: additive increase, multiplicative decrease</h3>

<p>Additive-increase/multiplicative-decrease (AIMD) is a feedback control algorithm that is the primary mechanism for adjusting the rate of a TCP flow, i.e. answering the question "How fast should I send?"</p>

<p>The basic idea of congestion control is that the sender transmits TCP packets on the network, then reacts to observable events to either increase or decrease its sending rate. If it believes the network is over-utilized, then it should decrease its sending rate; if it believes the network is under-utilized, then it should increase its sending rate. </p>

<p>There are various indicators of congestion that can signal that the network is over-utilized. Packet loss is one such indicator. When a sender transmits TCP packets at rate faster than the capacity of the <em>bottleneck</em> (the link with the smallest capacity in the path), the bottleneck router puts packets in a buffer. As the sender continues to transmit faster than packets can leave the buffer, the buffer will fill with packets until eventually there is no room for more packets. When the buffer is full, the router has no choice but to drop new packets as they arrive. The sender will realize that packets are lost when no ACK is received for them, and it will reduce its sending rate.</p>

<p>How does the sender reduce its sending rate? Most congestion control algorithm use a <em>congestion window</em> (CWND) to control sending rate. For each connection, TCP maintains a CWND that limits the total number of unacknowledged packets that may be in transit end-to-end ("bytes in flight"). In other words: if the number of unacknowledged segments is equal to the CWND, the sender stops sending data until more acknowledgements are received. </p>

<p>The CWND is not advertised or exchanged between the sender and receiver - it is a private value maintained locally by each end host. You can not find the CWND by inspecting packet headers.</p>

<p>When a connection is set up, the CWND is set to a small multiple of the maximum segment size (MSS) allowed on that connection. On modern Linux kernels, the initial CWND is 10 MSS. (Earlier versions used 4 MSS.)</p>

<p>The basic AIMD algorithm then goes as follows, assuming that packet loss is used as an indicator of congestion:</p>

<ul>
<li><strong>Additive increase</strong>: Increase the CWND by a fixed amount (e.g., one MSS) every round-trip time (RTT) that no packet is lost.</li>
<li><strong>Multiplicative decrease</strong>: Decrease the congestion window by a multiplicative factor (e.g., 1/2) every RTT that a packet loss occurs.</li>
</ul>

<p>Sending rate is not controlled entirely by the CWND, since TCP uses <em>flow control</em> in addition to <em>congestion control</em>. Flow control limits the number of unacknowledged segments according to the receive window size advertised by the receiver in packet headers. The rule is then: the maximum amount of data that may be in flight (i.e., not acknowledged) from the sender to the receiver is the minimum of the advertised receive window size (RWND) and CWND.</p>

<h3 id="slowstart">Slow start</h3>

<p>The pattern described above helps avoid over-utilization. However, under AIMD, it can take some time for a flow to actually reach link capacity, causing under-utilization of the link. Slow start was introduced to help TCP flows reach link capacity faster.</p>

<p>Slow start is in effect during the <em>congestion control</em> phase. During congestion control, CWND is increased by the number of segments acknowledged each time an ACK is received. This is exponential growth, rather than the slower linear growth of AIMD during the congestion avoidance phase. The congestion control phase continues until a loss event occurs, or until a <em>slow start threshold</em> is reached. </p>

<p>Once the slow start threshold is reached (or a packet loss is detected), then the TCP flow enters the congestion avoidance phase. At this point, AIMD kicks in and the CWND of the flow grows linearly.</p>

<p>How is the slow start threshold set? The slow start threshold is initialized to a large value at the beginning of the connection. Once the first loss event occurs, the slow start threshold is set to half of the CWND at the time of the loss event. </p>

<h3 id="fastrecovery">Fast recovery</h3>

<p>We mentioned that TCP can use an ACK timeout to detect lost segments. However, this means that a sender may not know that a segment is lost for a long time! Duplicate acknowledgements are an earlier indication of congestion. </p>

<p>Recall that an ACK indicates the sequence number of the last in-order byte of data received. When a segment X is lost but subsequent segments X+1, X+2, X+3, etc. are delivered successfully, the receiver will send an ACK for each subsequent segment. But the sequence number in the ACK of X+1, X+2, X+3, etc. will reflect the last segment that was received in order - the sequence number of X-1! Thus, the TCP sender will get "duplicate ACKs" - multiple ACKs with the same sequence number - and can conclude that the following segment was dropped.</p>

<p>In TCP Reno, the CWND decrease depends on whether congestion was detected by an ACK timeout or by receipt of duplicate ACKs:</p>

<ul>
<li>When there is an ACK timeout, the congestion is considered severe, since subsequent segments after the first dropped segment are also not getting through. TCP Reno reduces its CWND to a minimum value, and enters slow start.</li>
<li>When congestion is detected by the receipt of duplicate ACKs, the congestion is considered less severe. TCP Reno reduces its CWND to the slow start threshold, and enters congestion avoidance. This is known as "fast recovery".</li>
</ul>

<h3 id="overviewoftcpphases">Overview of TCP phases</h3>

<p>The following image illustrates the behavior of TCP in congestion control (slow start) mode and congestion avoidance mode.</p>

<p><img src="./TCP congestion control_files/TCP_Slow-Start_and_Congestion_Avoidance.svg" alt="">
<i>Image is GPLv3, via <a href="https://commons.wikimedia.org/wiki/File:TCP_Slow-Start_and_Congestion_Avoidance.svg">Wikimedia Commons</a>.</i></p>

<h3 id="animation">Animation</h3>

<p>The following animation shows AIMD congestion control at work in a network with a buffering router sitting between two hosts. Data (in blue) flows from the sending host at the left to the receiving host at the right; acknowledgements (in red) return over the reverse link. Utilization on the bottleneck link (from the router to the receiver) is noted. The plot at the bottom shows the sender congestion window as a function of time.</p>

<video controls="controls" width="800" height="600" webboost_found_paused="true" webboost_processed="true">  
    <source src="https://witestlab.poly.edu/respond/sites/genitutorial/files/tcp-aimd.ogv" type="video/ogg">
    <span title="No video playback capabilities, please visit the link below to view the animation.">Router Buffer Animation</span>
</video>  

<p><i>Animation Source: Guido Appenzeller and Nick McKeown, <a href="http://guido.appenzeller.net/anims/">Router Buffer Animations</a></i></p>

<p>As the congestion window increases, the rate at which (blue) packets are sent also increases, until the bottleneck link utilization reaches 100%. Then, as the rate of packets sent continues to increase, packets start to accumulate in the buffer. Eventually, the buffer becomes full and the router must drop new packets. </p>

<p>When the sender becomes aware of the dropped packet (because no ACK is received for it), it reduces its congestion window by a multiplicative factor. With a smaller congestion window, and many unacknowledged packets already "in flight", it must pause before it can resume transmission, so the buffer has a chance to drain. Once some time passes, and more of the "in flight" segments are acknowledged, the sender can resume transmission and begin to increase its congestion window again. This process continues for the lifetime of the flow, leading to a classic "sawtooth" pattern.</p>

<h2 id="results">Results</h2>

<p>In this experiment, we will send three TCP flows through a bottleneck link, and see the classic "sawtooth" pattern of the TCP congestion window, shown as the solid line in the plot below. The slow start threshold is shown as a dashed line, and instances of packet retransmission (due to lost packets) are shown as vertical bands:</p>

<p><img src="./TCP congestion control_files/sender-ss-1.svg" alt="">
<small><i>Figure 1: Congestion window size (solid line) and slow start threshold (dotted line) of three TCP flows sharing the same bottleneck.</i></small></p>

<p>We can also identify</p>

<ul>
<li>"Slow start" periods, where the congestion window increases rapidly.</li>
<li>"Congestion avoidance" periods (when the congestion window increases linearly from than the slow start threshold)</li>
<li>Instances where duplicate ACKs were received, if any. We are using <a href="http://intronetworks.cs.luc.edu/current/html/reno.html">TCP Reno</a>, which will enter "fast recovery" if it detects congestion by duplicate ACKs. The slow start threshold is set to half of the CWND at the time of the loss event, the new CWND is set to the slow start threshold, and the flow enters "congestion avoidance" mode.</li>
<li>Instances of ACK timeout, if any. This will cause the congestion window to go back to 1 MSS, and the flow enters "slow start" mode.</li>
</ul>

<p>For example, the following annotated image shows a short interval in one TCP flow:</p>

<p><img src="./TCP congestion control_files/tcp-one.svg" alt="">
<small><i>Figure 2: Events in a TCP flow. Slow-start periods are marked in yellow; congestion avoidance periods are in purple. We can also see when indicators of congestion occur: instances of timeout (red), and instances where duplicate ACKs are received (blue) triggering fast recovery (green).</i></small></p>

<h2 id="runmyexperiment">Run my experiment</h2>

<p>First, reserve a topology including two end hosts, and a router between them. The router will buffer traffic between the sender and the receiver. If the buffer in the router becomes full, it will drop packets, triggering TCP congestion control behavior.</p>

<p>Follow the instructions for the testbed you are using (GENI or Cloudlab,) to reserve the resources and log in to each of the hosts in this experiment. </p>

<div style="border-color:#FB8C00; border-style:solid; padding: 15px;">  
<h4 style="color:#FB8C00;"> GENI-specific instructions: Reserve resources</h4>

<p>In the GENI Portal, create a new slice, then click "Add Resources". Scroll down to where it says "Choose RSpec" and select the "URL" option, the load the RSpec from the URL: <a href="https://raw.githubusercontent.com/ffund/tcp-ip-essentials/gh-pages/rspecs/line-tso-off.xml">https://raw.githubusercontent.com/ffund/tcp-ip-essentials/gh-pages/rspecs/line-tso-off.xml</a>.</p>

<p>This will load the following topology in your canvas, with two hosts ("romeo" and "juliet") and a router connecting them:</p>

<img src="./TCP congestion control_files/tcp-topology.svg" alt="">

<p>Click on "Site 1" and choose an InstaGENI site to bind to, then reserve your resources. Wait for your nodes to boot up (they will turn green in the canvas display on your slice page in the GENI portal when they are ready). Then, click on "Details" to get SSH login information, and SSH into each node. </p>

<p>When you have logged in to each node, continue to the <a href="https://witestlab.poly.edu/blog/tcp-congestion-control-basics/#setupexperiment">Set up experiment</a> section.</p>

</div>  

<p><br></p>

<div style="border-color:#5e8a90; border-style:solid; padding: 15px;">  
<h4 style="color:#5e8a90;"> Cloudlab-specific instructions: Reserve resources</h4>

<p>To reserve these resources on Cloudlab, open this profile page: </p>

<p><a href="https://www.cloudlab.us/p/nyunetworks/education?refspec=refs/heads/tcp_congestion_control">https://www.cloudlab.us/p/nyunetworks/education?refspec=refs/heads/tcp_congestion_control</a></p>


<p>Click "next", then select the Cloudlab project that you are part of and a Cloudlab cluster with available resources. (This experiment is compatible with any of the Cloudlab clusters.) Then click "next", and "finish".</p>

<p>Wait until all of the sources have turned green and have a small check mark in the top right corner of the "topology view" tab, indicating that they are fully configured and ready to log in. Then, click on "list view" to get SSH login details for the client, router, and server hosts. Use these details to SSH into each.</p>

<p>When you have logged in to each node, continue to the <a href="https://witestlab.poly.edu/blog/tcp-congestion-control-basics/#setupexperiment">Set up experiment</a> section.</p>

</div>  

<p><br></p>

<h3 id="setupexperiment">Set up experiment</h3>

<p>On the end hosts ("romeo" and "juliet"), install the <code>iperf</code> network testing tool, with the command </p>

<pre><code>sudo apt-get update  
sudo apt-get -y install iperf3  
</code></pre>

<p>On romeo, we'll also install the <code>moreutils</code> utility, which will help us with data collection, and some other tools for data visualization:</p>

<pre>sudo apt-get -y install moreutils r-base-core r-cran-ggplot2 r-cran-littler
</pre>

<p>and configure an additional setting:</p>

<pre><code>sudo sysctl -w net.ipv4.tcp_no_metrics_save=1  
</code></pre>

<p>Also, configure the router as a 1 Mbps bottleneck, with a buffer size of 0.1 MB, in both directions:</p>

<pre>sudo tc qdisc del dev eth1 root
sudo tc qdisc add dev eth1 root handle 1: htb default 3
sudo tc class add dev eth1 parent 1: classid 1:3 htb rate 1Mbit
sudo tc qdisc add dev eth1 parent 1:3 handle 3: bfifo limit 0.1MB

sudo tc qdisc del dev eth2 root
sudo tc qdisc add dev eth2 root handle 1: htb default 3
sudo tc class add dev eth2 parent 1: classid 1:3 htb rate 1Mbit
sudo tc qdisc add dev eth2 parent 1:3 handle 3: bfifo limit 0.1MB
</pre>

<p>Don't worry if you see a message in the output that says</p>

<pre><code>RTNETLINK answers : No such file or directory  
</code></pre>

<p>This is normal, and not a problem!</p>

<h3 id="usingsstoobservetcpsocketstatistics">Using <code>ss</code> to observe TCP socket statistics</h3>

<p>The parameters of the TCP congestion control algorithm, such as congestion window and slow start threshold, are <em>not</em> included in any packet header, since these are used only on the sender side. Therefore, we can't see the details of the congestion control by monitoring packets in <code>tcpdump</code> or other packet capture tools.</p>

<p>Instead, we will use <code>ss</code>, a linux utility used to monitor local sockets and display socket statistics. In this section, we'll see what the <code>ss</code> output looks like, and what useful information it includes.</p>

<p>On the "juliet" host, set up an <code>iperf3</code> server with</p>

<pre><code>iperf3 -s  -1  
</code></pre>

<p>On the "romeo" host, initiate a connection to the <code>iperf3</code> server on "juliet" and send data using TCP Reno congestion control, for 60 seconds, with</p>

<pre><code>iperf3 -c juliet -t 60 -C reno  
</code></pre>

<p>While <code>iperf3</code> is still sending traffic, open another SSH session on "romeo" and run</p>

<pre><code>ss -ein dst 10.10.2.100  
</code></pre>

<p>Here, we use <code>ss</code> with some key arguments:</p>

<ul>
<li><code>-e</code> to show detailed socket information</li>
<li><code>-i</code> to show internal TCP information. This information is known only to the operating system at the sender side; it is not sent over the network or otherwise shared with the receiver.</li>
<li><code>-n</code> specifies that it should not try to resolve names, but should show numeric values (i.e. IP addresses and not hostnames)</li>
<li><code>dst 10.10.2.100</code> is a filter that says it should only show sockets with the remote address 10.10.2.100 (the address of "juliet")</li>
</ul>

<p>You can learn more about <code>ss</code> arguments with <code>man ss</code> or by visiting the online <a href="https://linux.die.net/man/8/ss">man page</a>. </p>

<p>The output of this command will look something like this, although the exact details will vary:</p>

<pre>Netid     State      Recv-Q      Send-Q            Local Address:Port             Peer Address:Port                                                                                                               
tcp       ESTAB      0           343176              10.10.1.100:49112             10.10.2.100:5201       timer:(on,1.204ms,0) uid:20001 ino:386735 sk:3b &lt;-&gt;
     ts sack reno wscale:7,7 rto:1212 rtt:833.978/7.108 mss:1448 pmtu:1500 rcvmss:536 advmss:1448 cwnd:66 ssthresh:35 bytes_acked:589374 segs_out:557 segs_in:273 data_segs_out:555 send 916.7Kbps lastsnd:8 lastrcv:5016 lastack:8 pacing_rate 1.2Mbps delivery_rate 942.8Kbps busy:4972ms rwnd_limited:96ms(1.9%) unacked:71 retrans:2/76 lost:2 sacked:5 rcv_space:14480 rcv_ssthresh:64088 notsent:240368 minrtt:0.483
tcp       ESTAB      0           0                   10.10.1.100:49110             10.10.2.100:5201       uid:20001 ino:386734 sk:3c &lt;-&gt;
     ts sack cubic wscale:7,7 rto:208 rtt:5.598/9.458 ato:40 mss:1448 pmtu:1500 rcvmss:536 advmss:1448 cwnd:10 bytes_acked:144 bytes_received:4 segs_out:8 segs_in:7 data_segs_out:3 data_segs_in:3 send 20.7Mbps lastsnd:5020 lastrcv:4972 lastack:4972 pacing_rate 41.4Mbps delivery_rate 9.2Mbps busy:48ms rcv_space:14480 rcv_ssthresh:64088 minrtt:0.723
</pre>

<p>We have two TCP sockets with the specified destination address. One is a socket used to share <code>iperf3</code> control information with the receiver. The other is a socket that carries the actual data between the sender and receiver. You can tell which one is which by looking at the <code>data_segs_out</code> value - the control flow only sends a few data segments, but the data flow will transfer hundreds or thousands of segments. In this case, the first lines in the <code>ss</code> output show the data flow, and the last lines show the control flow.</p>

<p>Also note that the data flow uses TCP Reno, as we specified in the <code>iperf3</code> arguments. The control flow uses whatever congestion control is the system default - here, it's TCP Cubic.</p>

<p>In the <code>ss</code> output for the data flow, see if you can find:</p>

<ul>
<li>the current CWND of this flow. This is shown in units of MSS.</li>
<li>the slow start threshold of this flow. This is shown in units of MSS. The slow start threshold field, <code>ssthresh</code>, will only appear in the <code>ss</code> output once the flow has entered the congestion avoidance phase.</li>
<li>the number of retransmitted segments. This will only appear in the <code>ss</code> output once there has been a retransmission in the lifetime of the flow. If it appears in the output, it will show two values: the number of currently unacknowledged retransmitted segments, and the total (cumulative) number of retransmissions for the flow.</li>
</ul>

<h3 id="generatingdata">Generating data</h3>

<p>Next, we will generate some TCP flows between the two end hosts, and use it to observe the behavior of the TCP congestion control algorithm.</p>

<p>While the TCP flows are running, we will also run a script that repeatedly runs <code>ss</code> and redirects the output to a file. When you press Ctrl+C, it will stop running the <code>ss</code> command and process the raw output into a format that is more convenient for data analysis and visualization.</p>

<p>Download this script on the "romeo" host with</p>

<pre>wget -O ss-output.sh https://raw.githubusercontent.com/ffund/tcp-ip-essentials/gh-pages/scripts/ss-output.sh
</pre>

<p>On the "juliet" host, run</p>

<pre><code>iperf3 -s -1  
</code></pre>

<p>In a terminal on the "romeo" host, run</p>

<pre><code>bash ss-output.sh 10.10.2.100  
</code></pre>

<p>In a second terminal on the "romeo" host, run</p>

<pre><code>iperf3 -c juliet -P 3 -t 60 -C reno  
</code></pre>

<p>Here</p>

<ul>
<li><code>-t 60</code> says to run for 60 seconds</li>
<li><code>-c juliet</code> says to send traffic to the host named "juliet"</li>
<li><code>-P 3</code> says to send 3 parallel TCP flows</li>
<li><code>-C reno</code> says to use TCP Reno for the data flows</li>
</ul>

<p>While <code>iperf3</code> is running, you will see periodic updates in the <code>iperf3</code> window,  and a continuous flow of socket statistics data in the <code>ss</code> window. After about a minute, you will see a final status report in the <code>iperf3</code> window, and the <code>iperf3</code> process will finish. Then, use Ctrl+C to stop the <code>ss</code> script in the other window. Pressing Ctrl+C once will cause the script to process the raw data, and then exit.  </p>

<p>If you run <code>ls</code> on the "romeo" host, you should see two files generated by the <code>ss</code> script:</p>

<ul>
<li><code>sender-ss.txt</code> is the raw output - the complete output of the <code>ss</code> command each time it was executed by the script.</li>
<li><code>sender-ss.csv</code> is a processed output - the script parsed the raw output, for each line of output, it prints the following comma-separated columns, in order, for each row of output:
<ul><li>Timestamp (in <a href="https://en.wikipedia.org/wiki/Unix_time">Unix time</a> format)</li>
<li>TCP sender, in IP:Port format. Each of the TCP flows in this experiment will use a different local port number, and the control flow will use a unique local port number as well. We'll use this field to distinguish the flows.</li>
<li>Number of currently unacknowledged retransmissions for this flow.</li>
<li>Cumulative number of retransmissions for this flow.</li>
<li>Current CWND of this flow.</li>
<li>Current slow start threshold of this flow.</li></ul></li>
</ul>

<p>You can transfer both of these files and the packet capture to your laptop with <code>scp</code>. </p>

<h3 id="visualization">Visualization</h3>

<p>You can use your preferred data visualization tool or programming language to analyze the results of your experiment. (Make sure to exclude the control flow from your analysis!) For convenience, I share an R script and a Python script here.</p>

<h4 id="rscriptfordatavisualization">R script for data visualization</h4>

<p>The script that I used to generate Figure 1 in the <a href="https://witestlab.poly.edu/blog/tcp-congestion-control-basics/#results">Results</a> section is written in R. On "romeo", run</p>

<pre>wget -O ss-data-analysis.R https://raw.githubusercontent.com/ffund/tcp-ip-essentials/gh-pages/scripts/ss-data-analysis.R
</pre>

<p>to retrieve the script, then</p>

<pre><code>Rscript ss-data-analysis.R  
</code></pre>

<p>to run it. You may see a warning message about removing rows containing missing values; that's OK! Run </p>

<pre><code>ls  
</code></pre>

<p>and you should see that it generated an image file named <code>sender-ss.svg</code>. Transfer this file to your computer with <code>scp</code>.</p>

<p>The results will look something like this:</p>

<p><img src="./TCP congestion control_files/sender-ss-2.svg" alt="">
<small><i>Figure 1 (same as Figure 1 in Results section): Congestion window size (solid line) and slow start threshold (dotted line) of three TCP flows sharing the same bottleneck.</i></small></p>

<p>The slow start threshold is shown as a dashed line, and instances of packet retransmission (due to lost packets) are shown as vertical bands:</p>

<p>At the beginning of each flow, it operates in slow start mode, where the congestion window increases exponentially. When a congestion event occurs, as indicated by the receipt of multiple duplicate ACKs, the slow start threshold (dashed line) is set to half of the current CWND, and then the CWND is reduces to the slow start threshold.</p>

<p>We'll often see packet losses occur at the same time in multiple flows sharing a bottleneck (as in the figure above), because when the buffer is full, new packets arriving from all flows are dropped. </p>

<h4 id="pythonscriptfordatavisualization">Python script for data visualization</h4>

<p>Alternatively, you can retrieve the <code>sender-ss.csv</code> file and plot it using Python. Here's a Python script:</p>

<pre>import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("sender-ss.csv", names=['time', 'sender', 'retx_unacked', 'retx_cum', 'cwnd', 'ssthresh'])

# exclude the "control" flow
s = df.groupby('sender').size()
df_filtered = df[df.groupby("sender")['sender'].transform('size') &gt; 100]

time_min = df_filtered.time.min()
cwnd_max = 1.1*df_filtered[df_filtered.time - time_min &gt;=2].cwnd.max()
dfs = [df_filtered[df_filtered.sender==senders[i]] for i in range(3)]

fig, axs = plt.subplots(len(senders), sharex=True, figsize=(12,8))
fig.suptitle('CWND over time')
for i in range(len(senders)):
    if i==len(senders)-1:
        axs[i].plot(dfs[i]['time']-time_min, dfs[i]['cwnd'], label="cwnd")
        axs[i].plot(dfs[i]['time']-time_min, dfs[i]['ssthresh'], label="ssthresh")
        axs[i].set_ylim([0,cwnd_max])
        axs[i].set_xlabel("Time (s)");
    else:
        axs[i].plot(dfs[i]['time']-time_min, dfs[i]['cwnd'])
        axs[i].plot(dfs[i]['time']-time_min, dfs[i]['ssthresh'])
        axs[i].set_ylim([0,cwnd_max])


plt.tight_layout();
fig.legend(loc='upper right', ncol=2);
</pre>

<h2 id="notes">Notes</h2>

<h3 id="exercise">Exercise</h3>

<p>Create a plot of the congestion window size and slow start threshold for each TCP flow over the duration of the experiment, similar to Figure 1 in the <a href="https://witestlab.poly.edu/blog/tcp-congestion-control-basics/#results">Results</a> section.</p>

<p>Annotate your plot, similar to Figure 2 in the <a href="https://witestlab.poly.edu/blog/tcp-congestion-control-basics/#results">Results</a> section, to show:</p>

<ul>
<li>Periods of "Slow Start" </li>
<li>Periods of "Congestion Avoidance"</li>
<li>Instances where multiple duplicate ACKs were received (which will trigger "fast recovery")</li>
<li>Instances of timeout (if any)</li>
</ul>

<p>Using your plot and/or experiment data, explain how the behavior of TCP is different in the "Slow Start" and "Congestion Avoidance" phases. Also, using your plot, explain what happens to both the congestion window and the slow start threshold when multiple duplicate ACKs are received.</p>

<h3 id="additionalexercisesothercongestioncontrolalgorithms">Additional exercises: other congestion control algorithms</h3>

<p>In the decades since TCP Reno was first proposed, several other congestion control algorithms have been developed that offer improved performance in some circumstances.</p>

<p>You can repeat this experiment with other congestion control variants by changing the value of the <code>-C</code> argument in <code>iperf3</code>. </p>

<p>First, make sure to save the data from the main part of this experiment. When you run the experiment again (with a different control control algorithm), it will overwrite your previous data, so you want to have that safely stored somewhere else first.</p>

<p>For example, you could run this experiment with TCP Cubic, which is the current default on Linux servers that power much of the Internet. The main difference between TCP Reno and TCP Cubic is the window increase function. While Reno uses the traditional linear increase (W=W+1), Cubic implements a binary search increase which can reach the available bandwidth much faster than Reno. You may read more about Cubic in the <a href="https://www.cs.princeton.edu/courses/archive/fall16/cos561/papers/Cubic08.pdf">TCP Cubic paper</a>.</p>

<p>To run the experiment with TCP Cubic, you would repeat the steps in the <a href="https://witestlab.poly.edu/blog/tcp-congestion-control-basics/#generatingdata">Generating Data</a> section above, but with the <code>iperf3</code> command</p>

<pre><code>iperf3 -c juliet -P 3 -t 60 -C cubic
</code></pre>

<p>on "romeo".</p>

<p>The results will look something like this:</p>

<p><img src="./TCP congestion control_files/sender-ss-cubic.svg" alt=""></p>

<p>Notice that unlike Reno, the window size does not increase as a linear function of the time since the last congestion event! Instead, the window size is a cubic function of the time since the last congestion event.</p>

<h3 id="additionalexerciseslowdelaycongestioncontrol">Additional exercises: low delay congestion control</h3>

<p>While TCP CUBIC and Reno are designed with the goal of high throughput, they tend to cause high queuing delays in the network, because they reduce their CWND only when they experience a packet loss, i.e. when the queue is full. A full queue means long queuing delays for packets that traverse the queue! </p>

<p>Some congestion control variants use delay as a signal of congestion, and reduce their sending rate when the delay increases (indicating that the queue is becoming full). An early example is TCP Vegas. You can see this for yourself with a simple experiment to measure the queuing delay with a loss-based congestion control (like Reno or Cubic) and with a delay-based congestion control (Vegas).</p>

<p>For this experiment we will use <code>iperf3</code> and <code>ping</code> at the same time - <code>iperf3</code> to generate a TCP flow, and <code>ping</code> to estimate the queuing delay induced by the TCP flow.  With an <code>iperf3</code> server running on juliet, in one terminal on romeo run</p>

<pre><code>iperf3 -c juliet -t 60 -C reno
</code></pre>

<p>and at the same time, in a second terminal on romeo, run</p>

<pre><code>ping juliet -c 50  
</code></pre>

<p>(the <code>ping</code> should both start and finish while the <code>iperf3</code> sender is still running). When it finishes, make a note of the <code>iperf3</code> throughput and the round trip time estimated by <code>ping</code> during the TCP Reno flow.</p>

<p>Then, repeat with Vegas, the delay-based congestion control algorithm. With an <code>iperf3</code> server running on juliet, in one terminal on romeo run</p>

<pre><code>sudo modprobe tcp_vegas  
sudo iperf3 -c juliet -t 60 -C vegas
</code></pre>

<p>and at the same time, in a second terminal on romeo, run</p>

<pre><code>ping juliet -c 50  
</code></pre>

<p>Make a note of the <code>iperf3</code> throughput and the round trip time estimated by <code>ping</code> during the TCP Vegas flow.</p>

<p>One problem with TCP Vegas is that it does not work well when it shares a bottleneck link with a TCP Reno flow (or other loss-based flow). To see how this works, we will send two TCP flows through the bottleneck router: one TCP Reno flow, and one TCP Vegas flow.</p>

<p>We will need two <code>iperf3</code> servers running on juliet, on two different ports. In one terminal on juliet, run</p>

<pre><code>iperf3 -s -1  
</code></pre>

<p>to start an <code>iperf3</code> server on the default port 5201, and in a second terminal on juliet, run</p>

<pre><code>iperf3 -s -1 -p 5301  
</code></pre>

<p>to start an <code>iperf3</code> server on port 5301.</p>

<p>You'll need two terminal windows on romeo. In one of them, run</p>

<pre><code>sudo iperf3 -c juliet -t 60 -C vegas  
</code></pre>

<p>and a few seconds afterwards, in the second, run</p>

<pre><code>iperf3 -c juliet -t 60 -C reno -p 5301  
</code></pre>

<p>Make a note of the throughput reported by <code>iperf3</code> for each flow.</p>

<h3 id="additionalexercisestcpbbr">Additional exercises: TCP BBR</h3>

<p>A more recent congestion control proposed by Google, called TCP BBR, tries to maximize throughput and at the same time minimize queuing delay in the network. You can read more about it in the <a href="https://research.google/pubs/pub45646/">TCP BBR paper</a>.</p>

<p>To use the BBR congestion control for your experiment, on romeo, run</p>

<pre><code>sudo modprobe tcp_bbr  
</code></pre>

<p>This will load the Linux kernel module for TCP BBR. </p>

<p>Then, repeat the other steps in the <a href="https://witestlab.poly.edu/blog/tcp-congestion-control-basics/#generatingdata">Generating Data</a> section above, but with the <code>iperf3</code> command</p>

<pre><code>iperf3 -c juliet -P 3 -t 60 -C bbr
</code></pre>

<p>on "romeo".</p>

<p>BBR doesn't use a slow start threshold, so you won't be able to use the same data visualization script (which assumes that there will be an <code>ssthresh</code> field in the data), but you can create a similar plot on your own. The results will look something like this:</p>

<p><img src="./TCP congestion control_files/sender-ss-bbr.svg" alt=""></p>

<p>Note that BBR overall maintains a lower CWND than Cubic or Reno, because it wants to minimize queue occupancy. But you'll see in the <code>iperf3</code> output that it still achieves a similar throughput (about 1Mbps total shared between the 3 flows). Also, if you look at the raw <code>ss</code> data for the BBR and the Reno/Cubic flows, you'll note that the BBR flows see a much lower RTT, since they do not fill the queue.</p>

<h3 id="additionalexercisesexplicitcongestionnotificationecn">Additional exercises: Explicit congestion notification (ECN)</h3>

<p>Finally, we'll try an experiment with explicit congestion notification. Explicit congestion notification (ECN) is a feature that allows routers to explicitly signal to a TCP sender when there is congestion. This allows the sender to reduce its congestion window <em>before</em> the router is forced to drop packets, reducing retransmissions. It can also help the router maintain a minimal queue, which reduces queuing delay.</p>

<p>ECN involves both layer 2 and layer 3, and it requires support from both transport layer endpoints (sender <em>and</em> receiver) and routers along the path traversed by the packets.</p>

<p>We will use ECN together with active queue management, which monitors the queuing delay. At the router, configure a queue in both directions that will mark packets when the queuing delay exceeds 10ms:</p>

<pre>sudo tc qdisc del dev eth1 root  
sudo tc qdisc add dev eth1 root handle 1: htb default 3  
sudo tc class add dev eth1 parent 1: classid 1:3 htb rate 1Mbit  
sudo tc qdisc add dev eth1 parent 1:3 handle 3:  codel limit 100 target 10ms ecn

sudo tc qdisc del dev eth2 root  
sudo tc qdisc add dev eth2 root handle 1: htb default 3  
sudo tc class add dev eth2 parent 1: classid 1:3 htb rate 1Mbit  
sudo tc qdisc add dev eth2 parent 1:3 handle 3:  codel limit 100 target 10ms ecn
</pre>

<p>On romeo and juliet, enable ECN in TCP by running</p>

<pre><code>sudo sysctl -w net.ipv4.tcp_ecn=1  
</code></pre>

<p>Next, we'll prepare to capture the TCP flow. On both end hosts, romeo <em>and</em> juliet, run:</p>

<pre><code>sudo tcpdump -s 80 -i eth1 'tcp' -w $(hostname -s)-tcp-ecn.pcap  
</code></pre>

<p>ECN uses two flags in the TCP header: the ECN Echo (ECE) flag, and the Congestion Window Reduced (CWR) flag. It also uses two ECN bits in the DiffServ field of the IP header. Here is how these header fields are used:</p>

<ul>
<li>During the connection establishment phase of TCP, both endpoints indicate to the other that they support ECN. First, one host sends an ECN-setup SYN packet: it sets the ECE and CWR flags in the TCP header of the SYN. Then, the other host response with an ECN-setup SYN-ACK packet: it sets the ECE flag (but not the CWR flag) in the TCP header of the SYN-ACK.</li>
<li>In any subsequent packets that carry data (not pure ACKs!), the sender will set the two ECN bits in the IP header to either 10 or 01. Either of these flag values will indicate to the routers along the path that this data packet uses an ECN-capable transport. </li>
<li>If the router wants to signal to the TCP sender that there is congestion - for example, if the queue at the router is starting to fill up - then it sets the two ECN bits in the IP header to 11 before forwarding the packet to the destination. This is a "Congestion Experienced" signal.</li>
<li>If the receiver gets a data packet with the CE signal (the ECN bits in the IP header are set to 11), the receiver will set the ECN-Echo (ECE) flag in the TCP header <em>of the ACK</em> for that packet.</li>
<li>When the sender gets the ACK with the ECE flag set, it will reduce its CWND. Then it will set the Congestion Window Reduced (CWR) flag in the TCP header of the  next packet.</li>
</ul>

<p>With the <code>tcpdump</code> running, we can now run the experiment. In a second terminal on juliet, run</p>

<pre><code>iperf3 -s -1  
</code></pre>

<p>In a second terminal on romeo, run</p>

<pre><code>iperf3 -c juliet -t 60 -C reno  
</code></pre>

<p>and finally, in a third terminal on romeo, run</p>

<pre><code>ping -c 50 juliet  
</code></pre>

<p>When the experiment finishes, compare the delay performance of Reno with ECN (this experiment) to your previous experiment showing the delay performance without ECN.</p>

<p>Also, transfer the packet captures to your laptop with <code>scp</code>, and look for the ECN-related fields in the IP header and TCP header, during connection establishment and during data transfer.</p>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>This post was updated in October 29, 2020, with contributions from Ashutosh Srivastava.</p>
        </section>

        <footer class="post-footer">


            <figure class="author-image">
                <a class="img" href="https://witestlab.poly.edu/blog/author/ffund/" style="background-image: url(/blog/content/images/2016/01/head.jpg)"><span class="hidden">Fraida Fund's Picture</span></a>
            </figure>

            <section class="author">
                <h4><a href="https://witestlab.poly.edu/blog/author/ffund/">Fraida Fund</a></h4>

                    <p>Read <a href="https://witestlab.poly.edu/blog/author/ffund/">more posts</a> by this author.</p>
                <div class="author-meta">
                    <span class="author-location icon-location">Brooklyn, NY</span>
                    <span class="author-link icon-link"><a href="http://witestlab.poly.edu/~ffund/">http://witestlab.poly.edu/~ffund/</a></span>
                </div>
            </section>


            <section class="share">
                <h4>Share this post</h4>
                <a class="icon-twitter" href="https://twitter.com/intent/tweet?text=TCP%20congestion%20control&amp;url=https://witestlab.poly.edu/blog/tcp-congestion-control-basics/" onclick="window.open(this.href, &#39;twitter-share&#39;, &#39;width=550,height=235&#39;);return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://witestlab.poly.edu/blog/tcp-congestion-control-basics/" onclick="window.open(this.href, &#39;facebook-share&#39;,&#39;width=580,height=296&#39;);return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=https://witestlab.poly.edu/blog/tcp-congestion-control-basics/" onclick="window.open(this.href, &#39;google-plus-share&#39;, &#39;width=490,height=530&#39;);return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>
<p>Did you reproduce this experiment? Have useful information to share with other intrepid researchers?
Post it here! Comments are posted following moderation.</p>
<script id="dsq-count-scr" src="./TCP congestion control_files/count.js.download" async=""></script>
<div id="disqus_thread"><iframe id="dsq-app8635" name="dsq-app8635" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" width="100%" src="./TCP congestion control_files/saved_resource.html" style="width: 1px !important; min-width: 100% !important; border: none !important; overflow: hidden !important; height: 596px !important;" horizontalscrolling="no" verticalscrolling="no"></iframe></div>
<script>
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = https://witestlab.poly.edu/blog/tcp-congestion-control-basics/; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = ''; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//witestlab.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


        </footer>

    </article>
</main>

<aside class="read-next">
    <a class="read-next-story " style="background-image: url(/blog/content/images/2017/04/youtube-fingerprint-grid-1.png)" href="https://witestlab.poly.edu/blog/de-anonymizing-tor-traffic-with-website-fingerprinting/">
        <section class="post">
            <h2>De-anonymizing Tor traffic with website fingerprinting</h2>
            <p>In this experiment, we will be setting up a private Tor network on GENI, and will be testing a…</p>
        </section>
    </a>
    <a class="read-next-story prev " style="background-image: url(/blog/content/images/2017/03/subnet-topology-2.svg)" href="https://witestlab.poly.edu/blog/designing-subnets/">
        <section class="post">
            <h2>Designing subnets</h2>
            <p>Your task in this experiment is to set up subnets in a few small LANs to meet given design…</p>
        </section>
    </a>
</aside>



        <footer class="site-footer clearfix">
            <section class="copyright"><a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="./TCP congestion control_files/88x31.png"></a><br>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.</section>

            <section class="poweredby">Proudly published with <a href="https://ghost.org/">Ghost</a></section>
        </footer>

    </div>

    <script type="text/javascript" src="./TCP congestion control_files/jquery-1.12.0.min.js.download"></script>
    
    <script type="text/javascript" src="./TCP congestion control_files/jquery.fitvids.js.download"></script>
    <script type="text/javascript" src="./TCP congestion control_files/index.js.download"></script>



<iframe style="display: none;" src="./TCP congestion control_files/saved_resource(1).html"></iframe></body></html>